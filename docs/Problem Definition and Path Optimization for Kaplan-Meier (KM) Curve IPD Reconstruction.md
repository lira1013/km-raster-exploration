**Problem Definition and Path Optimization for Kaplan-Meier (KM) Curve
IPD Reconstruction**

**Chen Lingyue, Biomarker Intelligent Agent Development Department,
Geneplus**

Objective: To provide a deliverable KM2IPD tool agent for the medical
school in support of the MarkerMind Agent Service project.

Project Status : This report documents the implementation of the
exploration and validation phases of IPD reconstruction based on KM
curves.

Some components (such as the R+Python hybrid stack) are presented as
validated design decisions and next-stage plans, rather than as a fully
deployed system.

I. Problem Definition: Format Depth Determines Processing Method

When reconstructing individual patient data (IPD), the primary task is
to identify the mathematical nature of the input source. Image formats
are not simply different suffixes, but rather the boundary between
\"probabilistic guesses\" and \"absolute facts.\"

Bitmap (Raster: PNG/JPG/HEIC): Images are composed of pixels. Algorithms
need to \"guess\" pixel connectivity amidst noise interference using
computer vision (CV) techniques, facing the challenge of artifacts
generated by format conversion.

Obtaining rasterKM graphs is easier; web pages (HTML previews), photos,
screenshots, and early paper scans (PDFs) are all bitmaps. Since bitmaps
are matrices of pixels, their extraction depends solely on image
recognition capabilities. Because image pixels lack \"true coordinate
semantics,\" various data settings and anti-crawler coordinate offsets
at the generation and publishing ends can be ignored. However, any step
in generating coordinates without external measurement support (such as
coordinate axis annotation, OCR recognition, and calculation) enters the
inference/prediction stage. This means the result is not \"real
measurement data,\" but rather an approximation of the model\'s output.
Therefore, using ordinary VLM or large language models to directly
output coordinate points based on visual understanding often produces
\"illusions\" or errors. In short: LLM/VLM can assist in recognizing
graphic structures, labels, text, and trend descriptions, but cannot be
the sole, precise \"numerical coordinate extractor.\" This is also the
practical approach in many document extraction technologies---using
OCR + dedicated CV algorithms + customized image parsing programs
instead of directly relying on VLM.

Vector (SVG/PDF): Images remain clear even when infinitely zoomed in.
This type of information is characterized by \"strong constraints.\"
Path data and absolute coordinates are stored directly within the file.
Extracting data from vector graphics involves directly obtaining the
actual coordinates.

However, obtaining vector data is severely limited, restricted to
databases and journal articles. Furthermore, the geometric and semantic
organization of graphs varies across different sources, requiring
tailored processing. Since vector data is essentially structured data,
multimodal large-scale language models cannot truly \"see\" it, often
resulting in errors. The only solution is to provide the XML text code
of the vector data to a regular dialogue model, relying on its code
understanding capabilities for extraction. However, a complex graph
source code can have tens of thousands of lines, and directly sending it
to an AI would consume a large number of tokens, potentially exceeding
character limits. Therefore, if the actual vector data from the
literature can be obtained, extracting coordinates using regular
expressions, tailored to the characteristics of the journal or database,
is the easiest approach.

In summary, in terms of computational differences, bitmap digitization
is an estimation problem, while vector parsing is an exact decoding
problem.

**Comparison of data modal characteristics and engineering practice:**

  ---------------- ---------------------------- -----------------------------
  characteristic   Bitmap (Raster:              Vector (SVG/PDF)
                   PNG/JPG/HEIC)                

  The essence of   Pixel matrix, lacking direct Path/vector coordinates,
  data             coordinate semantics         directly storing absolute
                                                coordinates

  Data acquisition Easy: Screenshot, photo,     Difficult: Available only in
  difficulty       webpage preview, early       databases or journals;
                   scanned documents            sources are limited.

  Visualization    Magnification may result in  Still clear even when
  features         blurriness or artifacts.     magnified infinitely

  Extraction       CV + OCR + Curve Fitting,    Parsing XML/Path data allows
  method           Inferring Coordinates Based  direct reading of
                   on Pixels                    coordinates.

  Data determinism Low, dependent on algorithm  High, as long as the parsing
                   and recognition accuracy     rules are correct.

  LLM/VLM usage    It can help identify graphic Vector graphics cannot be
                   structures, labels, and      directly \"seen\"; code/path
                   trends, but it cannot        parsing can be used as an
                   reliably generate precise    aid.
                   coordinates.                 

  Potential        Noise, artifacts, and format The source code is complex
  problems         conversion can cause errors. and has many lines; directly
                                                inputting it into AI consumes
                                                a large number of tokens.

  Best Practices   Reproducible coordinate      Obtain the actual vector
                   reconstruction is achieved   image source and extract the
                   using algorithmic inference  coordinates using a program
                   and OCR annotation.          or regular expressions based
                                                on the format.
  ---------------- ---------------------------- -----------------------------

Error mode depth comparison :

  ----------------- -------------------------- ------------------------------
  Dimension         Raster (based on           Vector
                    field/random measurement)  (object-based/deterministic
                                               parsing)

  The nature of     Pixel intensity field:     Explicit geometric primitives:
  information       Discrete sampling of a     coordinate points and paths
                    continuous signal.         defined parametrically.

  Core Error Model  Measurement noise includes Parsing Error: Only applies to
                    environmental              floating-point precision or
                    interference, segmentation path closure logic.
                    errors, and quantization   
                    skew.                      

  Error             Heteroscedasticity: Error  Deterministic: The error
  distribution      fluctuations are severe in distribution is extremely flat
  characteristics   densely packed areas of    and predictable.
                    the curve.                 

  Signal-to-noise   Resolution-sensitive: As   Resolution-independent: Core
  ratio (SNR)       PPI decreases, the         geometric feature information
                    effective signal is        is not lost regardless of the
                    quickly buried in noise.   scaling factor.

  Algorithm         Estimation problem:        Extraction: \"Read\"
  Paradigm          \"guessing\" the curve     coordinates based on rules and
                    based on probability       syntax parsing.
                    distribution and threshold 
                    judgment.                  

  computational     \$O(N \\times M)\$: Driven \$O(K)\$: Driven by feature
  complexity        by image size (total       complexity (number of path
                    number of pixels).         instructions).

  Data consistency  Untraceable: The same      Strong consistency: The
                    image yields different     parsing results for the same
                    results under different    XML structure are strictly
                    thresholds.                unique.

  Failure Mode      Bias in lifespan           Parsing interruption (crash)
                    calculation caused by edge caused by abnormal format.
                    blurring.                  
  ----------------- -------------------------- ------------------------------

2.  Current core ideas and common tools for handling KMTO IPD

Currently, the general steps for KM coordinate reconstruction of IPD
are: image recognition---coordinate extraction---coordinate-based IPD
reconstruction.

Regarding data reconstruction, NICE (National Institute for Health and
Care Evaluation and Research) explicitly recommends in its DSU Technical
Support Document 19 that, when performing \"corrected indirect treatment
comparisons (MAIC),\" if the original data is unavailable, the method of
Guyot et al. should be used to reconstruct IPD. This is a widely
accepted and recognized method in academia. To date, Guyot and its
extended IPD reconstruction algorithms, such as those developed by Basia
et al., can accurately reconstruct events and censored coordinates into
IPD data, and are applied in various academic research projects.
However, the reliability of the reconstructed data depends on the input
coordinate data and may be affected by user errors during the
digitization process. For example, consistently marking digitized points
too low or too high may lead to biases in the reconstructed data. This
is an inherent limitation of all Individual Patient Data (IPD)
reconstruction algorithms.

For image recognition and coordinate acquisition, the most successful
tool remains the human-in-the-loop tool: WebPlotDigitizer . Its
extensive experience base and manually corrected patterns have gained
academic recognition. Earlier tools, such as DigitizeIt/PlotDigitizer ,
also fail to provide manual intervention.

## SurvdigitizeR, which has emerged in recent years, is one of the more mature fully automatic/semi-automatic KM curve image digitization tools available. Its drawback is that it relies on the R language instead of Python. If used as the core of curve digitization for intelligent agents, its startup overhead, concurrency support, and integration limitations are its biggest disadvantages.

The latest version, KMGPT, is not open source. It claims to achieve
end-to-end identification as a KMtoipd agent, but requires an OPENAI API
token.

Kaplan-Meier (KM) curve IPD reconstruction projects without human
intervention , the biggest and insurmountable challenge is the Python
algorithm for curve digitization.

+:-----------------+:---------------+:-----------------+:------------------+:---------------+:-------------------+:---------------+:---------------+
| Process Steps    | Target         | Common tools /   | Automation level  | language       | advantage          | Disadvantages  | Applicable     |
|                  |                | methods          |                   |                |                    | / Limitations  | Scenarios      |
+------------------+----------------+------------------+-------------------+----------------+--------------------+----------------+----------------+
| Image            | Identify       | WebPlotDigitizer | Semi-automatic /  | Web/JS         | Mature,            | Manual         | Academic       |
| recognition and  | curves, axes,  |                  | Human-in-the-loop |                | experienced, and   | confirmation   | research,      |
| coordinate       | ticks, and     |                  |                   |                | capable of manual  | is required;   | digital        |
| extraction       | labels from KM |                  |                   |                | correction         | it cannot      | experience     |
|                  | curve images.  |                  |                   |                |                    | completely     | database       |
|                  |                |                  |                   |                |                    | free up your   |                |
|                  |                |                  |                   |                |                    | hands.         |                |
+------------------+----------------+------------------+-------------------+----------------+--------------------+----------------+----------------+
|                  |                | DigitizeIt /     | semi-automatic    | C++ / GUI      | Established tool,  | It cannot be   | Early          |
|                  |                | PlotDigitizer    |                   |                | can manually       | automated and  | literature or  |
|                  |                |                  |                   |                | extract            | involves a lot | manual         |
|                  |                |                  |                   |                | coordinates        | of repetitive  | digitization   |
|                  |                |                  |                   |                |                    | work.          |                |
+------------------+----------------+------------------+-------------------+----------------+--------------------+----------------+----------------+
|                  |                | SurvdigitizeR    | Semi-automatic /  | R              | It can handle      | It relies on   | High           |
|                  |                |                  | near-fully        |                | pixel images and   | R, has high    | automation     |
|                  |                |                  | automatic         |                | curve separation,  | startup        | requirements,  |
|                  |                |                  |                   |                | and is relatively  | overhead, poor | but R          |
|                  |                |                  |                   |                | mature.            | concurrency,   | environment is |
|                  |                |                  |                   |                |                    | and complex    | available.     |
|                  |                |                  |                   |                |                    | integration.   |                |
+------------------+----------------+------------------+-------------------+----------------+--------------------+----------------+----------------+
|                  |                | Self-built       | Fully automated   | Python         | It can embed       | Algorithm      | Highly         |
|                  |                | Python pipeline  | (expandable)      |                | intelligent        | development    | automated      |
|                  |                | (OpenCV + OCR +  |                   |                | agents, requiring  | and debugging  | KM→IPD system  |
|                  |                | Clustering)      |                   |                | no human           | are required;  |                |
|                  |                |                  |                   |                | intervention and   | handling       |                |
|                  |                |                  |                   |                | is prone to        | low-quality    |                |
|                  |                |                  |                   |                | concurrency.       | images and     |                |
|                  |                |                  |                   |                |                    | overlapping    |                |
|                  |                |                  |                   |                |                    | curves is      |                |
|                  |                |                  |                   |                |                    | challenging.   |                |
+------------------+----------------+------------------+-------------------+----------------+--------------------+----------------+----------------+
| Coordinate-based | Reconstructing | Guyot et al.,    | Fully automatic   | R              | Academically       | Accuracy       | Academic MAIC, |
| IPD              | individual     | 2012 Methods     | (after coordinate | implementation | recognized,        | depends on     | Meta-analysis, |
| reconstruction   | patient data   |                  | input)            |                | capable of         | coordinate     | Systematic     |
|                  | from digital   |                  |                   |                | reconstructing     | input; user    | Reviews        |
|                  | coordinate     |                  |                   |                | events/censorship, | digitization   |                |
|                  | points         |                  |                   |                | and widely cited.  | errors will    |                |
|                  |                |                  |                   |                |                    | affect the     |                |
|                  |                |                  |                   |                |                    | results.       |                |
+------------------+----------------+------------------+-------------------+----------------+--------------------+----------------+----------------+
|                  |                | Basia and other  | Fully automatic   | R              | More granular and  | Coordinate     | High-precision |
|                  |                | extended         |                   |                | scalable           | dependency is  | scene          |
|                  |                | algorithms       |                   |                | event/censorship   | the same as    | reconstruction |
|                  |                |                  |                   |                | handling           | above.         |                |
+------------------+----------------+------------------+-------------------+----------------+--------------------+----------------+----------------+
|                  |                | KMGPT            | End-to-end        | Depends on     | Theoretically, it  | It is not open | Experimental   |
|                  |                |                  | intelligent       | Open           | is possible to     | source,        | end-to-end     |
|                  |                |                  | agents,           |                | directly recognize | consumes       | KM→IPD agent   |
|                  |                |                  | automation        | AI API         | images and perform | OpenAI tokens, |                |
|                  |                |                  |                   |                | multiple re-image  | and its        |                |
|                  |                |                  |                   |                | recognition.       | implementation |                |
|                  |                |                  |                   |                |                    | is not         |                |
|                  |                |                  |                   |                |                    | auditable.     |                |
+------------------+----------------+------------------+-------------------+----------------+--------------------+----------------+----------------+

3.  Algorithm Design and Experimental Verification

To build a stable and generalizable Kaplan--Meier (KM) curve
digitization system, this project did not rely directly on a single
off-the-shelf algorithm. Instead, it adopted a systematic algorithm
design space exploration + empirical validation strategy to gradually
compare the applicability of different technical routes and finally
converge to the current topology-first analytical framework.

This section summarizes the key experimental findings from the data
construction, pipeline disassembly, and selection of critical methods.

### 1. Synthetic Benchmark Construction

By introducing real clinical survival data from cBioPortal as the
underlying ground truth, a programmatically synthesized image dataset
with multidimensional perturbation characteristics was achieved,
eliminating the evaluation dilemmas caused by \'uncontrollable
signal-to-noise ratio\' and \'missing reference answers\' in real
literature images . Therefore, a synthetic benchmark dataset was
constructed to support algorithm iteration and repeatable testing.

The data source is over 20 real single-arm/two-arm survival datasets
from cBioPortal. 270 KM curve images were generated programmatically.
Coverage included different color schemes, line widths, noise levels,
and resolutions for single-arm, two-arm, and three-arm models.
Simultaneously, 30 real breast cancer HRD control two-arm survival
curves were generated. Additionally, YOLO annotations (for ROI/curve
detection) were generated concurrently. The accuracy was verified by
comparing the results with the agent\'s ROI node results.

The purpose of constructing this dataset is to achieve a quantitative
analysis of the algorithm\'s accuracy by building a benchmark framework
consisting of 300 synthetic curves and YOLO annotations. This not only
provides data proof for Python to replace R, but also establishes a
reproducible and measurable technical access standard for the entire
digital survival analysis business.

### 2. Modular Decomposition of Digital Processes (Pipeline Decomposition)

Research and observation revealed that KM curve digitization is not a
simple image processing problem, but rather involves: geometric
localization , image enhancement , topological restoration , event
parsing , coordinate mapping , and statistical reconstruction .
Therefore, the overall task was broken down into seven independent
modules:

\(1\) ROI clipping and coordinate alignment (ROI & Offsetting)

\(2\) Gray-scale priority image enhancement

\(3\) Topology-preserving skeletonization

\(4\) Start Region Merging

\(5\) Run-length / Topology event parsing (Event Logic Parsing)

\(6\) Pixel-mathematical coordinate mapping

\(7\) Risk set inversion and KM statistical reconstruction

The advantages of this modular design include support for independent
debugging , algorithm replacement and comparison , easy error
attribution , and support for ablation study , which significantly
improves the engineering maintainability and scalability of the system.

### 3. Algorithm Route Comparison and Experimental Findings (Empirical Findings & Ablation Study)

During the development process, a variety of common image processing
approaches were compared and tested.

### **3.1 Edge detection-based methods (Canny / Sobel)**

Assumption: The KM curve can be regarded as the image edge and can be
directly extracted through edge detection.

Experimental observation: The curve is rendered with thick lines and
anti-aliasing . Therefore, a double edge is produced .

Pixel bitmaps are prone to breakage and fragmentation , and are
susceptible to noise, dashed lines, and censoring .

Result: Severe disruption of topological continuity led to a high
failure rate in subsequent reconstruction.

Conclusion: The sharp boundary assumption does not conform to the
characteristics of KM curve plotting and is not suitable as the main
method.

### **3.2 Based on morphology and skeletonization methods**

Hypothesis: It may be more stable to directly restore the topology of
the curve centerline instead of detecting the edges.

has good connectivity , the influence of noise is significantly reduced
, and the structure is more consistent with the characteristics of a
step-function .

Result: Improved for event point detection and easier for run-length
parsing.

Conclusion: Skeletonization + topology restoration better matches the
structural characteristics of the KM curve than edge detection.

### **3.3 Color/Statistical Modeling Methods**

Assumption: The KM curve is essentially a \"visualization of a
statistical function\" and should not rely entirely on gradient
features.

Experimental observations show that color separation combined with curve
fitting is significantly superior to the gradient method , effectively
distinguishing multi-armed curves and exhibiting stronger noise
resistance.

Conclusion: KM digitization is closer to a topology restoration +
statistical modeling problem than a traditional edge detection problem .

### Key Insights Summary

  ----------------- --------------------------- -----------------------------
  Dimension         Bitmap edge method          Topology-based methods
                    (Raster-based)              

  The essence of    Local pixel density jumps   Global pixel connectivity
  data              (gradients).                (Connection).

  Signal-to-noise   Easy to decline: greatly    Stability: Focus on the
  ratio (SNR)       affected by image blur and  skeletal structure and ignore
                    noise interference.         surface burrs.

  Connectivity      Easily broken: A slightly   Maintain integrity: Paths are
                    higher threshold will cause maintained through
                    the curve to become         disjoint-set data structures
                    discontinuous.              and Run parsing.

  Certainty         Probabilistic: Guessing the Decisive factor: Extracting
                    edge based on a threshold.  absolute coordinates based on
                                                geometric rules.

  Sources of error  Pixel noise (quantization   Numerical precision
                    error, ambient light).      (floating-point conversion,
                                                analytical logic).

  Algorithm         High: Requires processing N Low: Only processes feature
  complexity        times M full-image          points (Runs) of the core
                    convolutions.               skeleton.

  applicability     Poor: Difficult to handle   Better: It can perfectly
                    curve intersections or      reproduce the characteristics
                    extremely fine lines.       of the step-like survival
                                                function.
  ----------------- --------------------------- -----------------------------

The final system adopts a hybrid strategy of topology-first and
statistical reconstruction as its core implementation route.

IV\. Current Project Progress

![](media/image1.png){width="5.7625in" height="2.421527777777778in"}

A complete pipeline for dual-input types has been established. The
vector pipeline is based on the KM curve settings of the cbioportal
database, using JavaScript to convert the vector image into coordinate
JSON data. Theoretically, as long as the input is a KM curve from the
cbioportal library, its accuracy can be 100%. No bias was observed in 5
sample validations .

The raster link has already used Python to complete the mapping from
image to coordinate pixels and the mapping cleanup. The curve coordinate
generation node must be generated by an algorithm. The details of the
Python algorithm urgently need adjustment. After the algorithm is
completed, it can be connected to an agent to realize the KM
digitization stage and then transition to IPD reconstruction.

The two feasible solutions for generating nodes based on current curve
coordinates are:

A uses Python to re-digitize curves based on Guyot\'s algorithm.

BR Packaging API

Python digitization currently takes approximately 14 seconds per graph
(actual test), but this has not yet been verified.

![db306019-97f3-4fe5-a20e-da989136a258](media/image2.png){width="6.989583333333333in"
height="0.8076388888888889in"}\
The R pipeline is not yet benchmarked; cold start/QPS/latency testing
can be planned using wrk + plumber API.

V. Accurate Project Plan

Considering that the curve digitization step could utilize Python to
call R\'s SurvdigitizeR, and then encapsulate it with the Guyot
algorithm, the success rate is highest. It allows for rapid feasibility
verification and has the highest short-term success rate. However, in
the long term, R services have significant limitations in concurrency,
deployment complexity, and agent integration, so I prefer to use it as a
transitional solution. However, startup overhead, long-term maintenance,
concurrency support, and integration issues are unavoidable. My own
Python implementation offers the highest product value and lower
computational cost, but its algorithm implementation is very difficult,
unverified, has low academic credibility, and requires a long
mathematical survival analysis and debugging cycle. Therefore, based on
current experimental results and engineering constraints, I determine
that the R + Python Hybrid Stack is the optimal implementation path for
this problem at this stage. (Hybrid stack: R as the algorithm core +
Python as the system shell). It should be noted that this hybrid
architecture is currently in the solution verification and technical
decision-making stage and has not yet been fully implemented. Among
them: the Vector (cbioportal) path has been implemented and verified to
be usable; the Python digitization algorithm for the Raster path has
completed pipeline construction and partial module verification, but
there are still significant recognition errors under complex noise
conditions; to further advance the Raster path, it is necessary to
introduce mature statistical implementations (such as SurvdigitizeR) to
reduce academic and engineering risks.

Therefore, Hybrid Stack is positioned as a next-stage implementation
plan, rather than a currently completed system.

This approach balances the statistical maturity of R\'s algorithms with
the engineering sophistication of Python. While its performance isn\'t
optimal (slower call speed) and carries long-term maintenance risks,
these are negligible in the current scenario. At this stage (goals:
rapid verification + academic credibility prioritized), a hybrid
approach is chosen. The Hybrid Stack is the optimal solution at this
stage; once the Python digitization algorithm has been validated on at
least N benchmarks, the R core can be gradually replaced.

+:---------------+:----------------+:----------------+:---------------+:----------------------+
| Solution Name  | Core Logic      | Advantages      | Disadvantages  | Execution steps       |
|                |                 |                 | (Cons)         |                       |
+----------------+-----------------+-----------------+----------------+-----------------------+
| Option 1: Full | SurvdigitizeR + | It boasts the   | It has poor    | 1\. Setting up the R  |
| R stack        | Guyot           | fastest         | concurrency    | environment           |
|                |                 | verification    | support,       |                       |
|                |                 | speed and the   | complex        | 2\. Use SurvdigitizeR |
|                |                 | highest         | deployment,    | to extract            |
|                |                 | short-term      | and low agent  | coordinates.          |
|                |                 | success rate.   | integration.   |                       |
|                |                 |                 |                | 3\. Run the Guyot     |
|                |                 |                 |                | algorithm             |
|                |                 |                 |                |                       |
|                |                 |                 |                | 4\. Encapsulate as a  |
|                |                 |                 |                | REST API              |
+----------------+-----------------+-----------------+----------------+-----------------------+
| Option 2:      | Self-developed  | The product     | The algorithm  | 1\. In-depth research |
| Implementation | algorithm       | boasts the      | is difficult,  | on mathematical       |
| entirely in    | implementation  | highest value,  | has low        | models of survival    |
| Python         |                 | low computing   | academic       | analysis              |
|                |                 | power           | credibility,   |                       |
|                |                 | consumption,    | and requires a | 2\. Write Python      |
|                |                 | and perfect     | long debugging | digitization code     |
|                |                 | engineering     | period.        |                       |
|                |                 | integration.    |                | 3\. Large-scale       |
|                |                 |                 |                | verification of data  |
|                |                 |                 |                | accuracy              |
+----------------+-----------------+-----------------+----------------+-----------------------+
| Option 3:      | R (algorithm    | The current     | The call       | 1\. Use R to          |
| Hybrid Stack   | kernel) +       | optimal         | performance is | encapsulate the core  |
|                | Python (system  | solution.       | slightly low,  | algorithm as a local  |
|                | shell)          | Balancing       | and there is a | service.              |
|                |                 | algorithm       | risk of        |                       |
|                |                 | maturity with   | long-term      | 2\. Python is         |
|                |                 | engineering     | maintenance    | responsible for       |
|                |                 | sophistication. | issues due to  | request and logic     |
|                |                 |                 | cross-language | orchestration.        |
|                |                 |                 | limitations.   |                       |
|                |                 |                 |                | 3\. Package it into a |
|                |                 |                 |                | standard API          |
|                |                 |                 |                | interface agent       |
+----------------+-----------------+-----------------+----------------+-----------------------+

This work establishes a proven technical approach and identifies the
difficulties of bitmap link processing methods under real-world
constraints . Future work will focus on implementing the proposed
R+Python hybrid stack or moving towards direct sagittal graph
identification and acquisition as described in the literature to improve
product usability, robustness, and academic reproducibility.
